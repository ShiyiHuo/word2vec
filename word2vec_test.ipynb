{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Word2Vec\n",
    "=============\n",
    "\n",
    "The goal of this tutorial is to train a Word2Vec skip-gram model over http://mattmahoney.net/dc/text8.zip data. This tutorial is based on http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow \n",
    "\n",
    "First download the file (text8.zip) from the given link and put it in the same directory as this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0K1ZyLn04QZf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/fanfan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Modules we'll be using later. \n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import fileinput\n",
    "import os, sys\n",
    "import shutil\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zqz3XiqI4mZT"
   },
   "source": [
    "Read the data into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28844,
     "status": "ok",
     "timestamp": 1445964497165,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "e3a928b4-1645-4fe8-be17-fcf47de5716d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_of_words size is  989006\n",
      "list_of_words size is  989006\n",
      "Some words are  ['chelsey', 'holley', 'consult', 'screening', 'colonoscopy', 'history', 'a']\n"
     ]
    }
   ],
   "source": [
    "def extract_word_list():\n",
    "    list_of_words = []\n",
    "    directoris = [\"L_the_patient\", \"U_Patient\", \"U_The_patient\"]\n",
    "    genders = [\"female\", \"male\", \"universal\"]\n",
    "\n",
    "    for dir_index in directoris:\n",
    "        for gender_index in genders:\n",
    "            dir = \"letters/\"+dir_index+\"/\"+gender_index+\"/\"\n",
    "            path = os.fsencode(dir)\n",
    "\n",
    "            for file in os.listdir(path):\n",
    "                filename = os.fsdecode(file)\n",
    "                if filename != \".DS_Store\":\n",
    "                    f = open(dir+filename,'r')\n",
    "                    filedata = f.read()\n",
    "                    f.close()\n",
    "                    \n",
    "                    from nltk.tokenize import word_tokenize\n",
    "                    tokens = word_tokenize(filedata)\n",
    "                    words = [word for word in tokens if word.isalpha()] # remove punctuation\n",
    "                    lowercased = [word.lower() for word in words]\n",
    "                    list_of_words.append(lowercased)\n",
    "\n",
    "    flattened_list = [y for x in list_of_words for y in x]\n",
    "    print('list_of_words size is ', len(flattened_list))\n",
    "    return flattened_list\n",
    "\n",
    "list_of_words = extract_word_list()\n",
    "\n",
    "print('list_of_words size is ', len(list_of_words))\n",
    "print('Some words are ', list_of_words[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using *zipfile.ZipFile()* to extract the zipped file, we can then use the reader functionality found in this zipfile module.  First, the *namelist()* function retrieves all the members of the archive – in this case there is only one member, so we access this using the zero index.  Then we use the *read()* function which reads all the text in the file.  Finally, we use *split()* function to create a list with all the words in the text file, separated by white-space characters.  We can see some of the output above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can observe, the returned vocabulary data contains a list of plain English words, ordered as they are in the sentences of the original extracted text file.  Now that we have all the words extracted in a list, we have to do some further processing to enable us to create our skip-gram batch data.  These further steps are:\n",
    "\n",
    "1. Extract the top 10,000 most common words to include in our embedding vector\n",
    "2. Gather together all the unique words and index them with a unique integer value – this is what is required to create an equivalent one-hot type input for the word.  We’ll use a Python dictionary to do this\n",
    "3. Loop through every word in the dataset (vocabulary variable) and assign it to the unique integer word identified, created in Step 2 above.  This will allow easy lookup / processing of the word data stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 28849,
     "status": "ok",
     "timestamp": 1445964497178,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "3fb4ecd1-df67-44b6-a2dc-2291730970b2"
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 1000\n",
    "\n",
    "words_counts = Counter(list_of_words)\n",
    "\n",
    "top_words_counts = words_counts.most_common(vocabulary_size - 1)\n",
    "\n",
    "dictionary = dict(); data = list(); index = 0;\n",
    "\n",
    "for word, _ in top_words_counts:\n",
    "    dictionary[word] = index\n",
    "    data.append(index)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'the'), (1, 'and'), (2, 'was'), (3, 'of'), (4, 'to'), (5, 'a'), (6, 'with'), (7, 'in'), (8, 'is'), (9, 'patient'), (10, 'she'), (11, 'for'), (12, 'were'), (13, 'no'), (14, 'he'), (15, 'on'), (16, 'this'), (17, 'at'), (18, 'then'), (19, 'right')]\n"
     ]
    }
   ],
   "source": [
    "reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "\n",
    "#print('Most common words', top_words_counts[:5])\n",
    "#print('Sample data', data[:10])\n",
    "#print('Sample data', [reverse_dictionary[data[i]] for i in range(10)])\n",
    "print(list(reverse_dictionary.items())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['consult', 'colonoscopy', 'colonoscopy', 'colonoscopy', 'history', 'history', 'history', 'a', 'a', 'a', 'a', 'is', 'is', 'is', 'is', 'a', 'a', 'a', 'a', 'who', 'who', 'who', 'i', 'i', 'i', 'well', 'well', 'well', 'because', 'because', 'because', 'i', 'i', 'i', 'i', 'have', 'have', 'have', 'have']\n",
      "['colonoscopy', 'consult', 'history', 'a', 'colonoscopy', 'a', 'is', 'colonoscopy', 'history', 'is', 'a', 'history', 'a', 'a', 'who', 'a', 'is', 'who', 'i', 'is', 'a', 'i', 'a', 'who', 'well', 'i', 'because', 'i', 'well', 'i', 'have', 'well', 'because', 'have', 'been', 'because', 'i', 'been', 'taking']\n",
      "[410, 808, 808, 808, 26, 26, 26, 5, 5, 5, 5, 8, 8, 8, 8, 5, 5, 5, 5, 82, 82, 82, 39, 39, 39, 37, 37, 37, 293, 293, 293, 39, 39, 39, 39, 45, 45, 45, 45]\n",
      "[808, 410, 26, 5, 808, 5, 8, 808, 26, 8, 5, 26, 5, 5, 82, 5, 8, 82, 39, 8, 5, 39, 5, 82, 37, 39, 293, 39, 37, 39, 45, 37, 293, 45, 57, 293, 39, 57, 526]\n"
     ]
    }
   ],
   "source": [
    "word_index = 0\n",
    "\n",
    "def generate_batch(batch_size=16, skip_window=2):\n",
    "    global word_index   # now we can change this variable inside this function\n",
    "    \n",
    "    batch  = []\n",
    "    labels = []\n",
    "    \n",
    "    for _ in range(batch_size): \n",
    "        if list_of_words[word_index] in dictionary:\n",
    "            left  = max(0, word_index-skip_window)\n",
    "            right = min(word_index+skip_window+1, len(list_of_words))\n",
    "            labels_indexes = list(range(left,word_index)) + list(range(word_index+1,right))\n",
    "\n",
    "            for i in labels_indexes:\n",
    "                if list_of_words[i] in dictionary:\n",
    "                    batch.append(dictionary[list_of_words[word_index]])\n",
    "                    labels.append(dictionary[list_of_words[i]])\n",
    "\n",
    "        word_index += 1\n",
    "    \n",
    "    return batch, labels\n",
    "\n",
    "batch, labels = generate_batch()\n",
    "\n",
    "print([reverse_dictionary[x] for x in batch])\n",
    "print([reverse_dictionary[x] for x in labels])\n",
    "print(batch)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ofd1MbBuwiva"
   },
   "source": [
    "Train a skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "8pQKsV4Vwlzy"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32)\n",
    "train_labels = tf.placeholder(tf.int32)\n",
    "\n",
    "# Look up embeddings for inputs.\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs) # train_inputs will be a batch at a time\n",
    "# tf.nn.embedding_lookup() is a useful helper function in TensorFlow. \n",
    "# Here’s how it works – it takes an input vector of integer indexes – in this case our train_input batch \n",
    "# of training input words, and \"looks up\" these indexes in the supplied embeddings tensor.  \n",
    "# Therefore, this command will return the current embedding vector for each of the supplied input words \n",
    "# in the training batch.  The full embedding tensor will be optimized during the training process.\n",
    "\n",
    "# Construct the variables for the softmax\n",
    "weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "hidden_out = tf.matmul(embed, tf.transpose(weights)) + biases\n",
    "\n",
    "# convert train_context to a one-hot format\n",
    "train_one_hot = tf.one_hot(train_labels, vocabulary_size)\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, labels=train_one_hot))\n",
    "# Construct the SGD optimizer using a learning rate of 1.0.\n",
    "optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 23
      },
      {
       "item_id": 48
      },
      {
       "item_id": 61
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 436189,
     "status": "ok",
     "timestamp": 1445965429787,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "1bQFGceBxrWW",
    "outputId": "5ebd6d9a-33c6-4bcd-bf6d-252b0b6055e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  7.04640007019\n",
      "Average loss at step  20 :  6.61802151203\n",
      "Average loss at step  40 :  6.23343007565\n",
      "Average loss at step  60 :  6.24429838657\n",
      "Average loss at step  80 :  5.98988995552\n",
      "Average loss at step  100 :  6.00317900181\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized')\n",
    "\n",
    "average_loss = 0\n",
    "num_steps = 101\n",
    "\n",
    "\n",
    "for step in range(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch()\n",
    "    # We need to convert these lists to numpy arrays. \n",
    "    # batch_inputs needs to be a row array.\n",
    "    # batch_labels needs to be a column array.  \n",
    "    length = len(batch_inputs)\n",
    "    batch_inputs = np.array(batch_inputs)\n",
    "    batch_labels = np.array(batch_labels).reshape(length, 1)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "    _, loss_val = session.run([optimizer, cross_entropy], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 20 == 0:\n",
    "        if step > 0:\n",
    "            average_loss /= 20\n",
    "        # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "        print('Average loss at step ', step, ': ', average_loss)\n",
    "        average_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_sampled = 64    # Number of examples to sample.\n",
    "\n",
    "nce_loss = tf.reduce_mean( \n",
    "        tf.nn.nce_loss(weights=weights,\n",
    "                       biases=biases,\n",
    "                       labels=train_labels,\n",
    "                       inputs=embed,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=vocabulary_size))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(nce_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  143.895339966\n",
      "Average loss at step  2000 :  nan\n",
      "Average loss at step  4000 :  nan\n",
      "Average loss at step  6000 :  nan\n",
      "Average loss at step  8000 :  4.7423764149\n",
      "Average loss at step  10000 :  nan\n"
     ]
    }
   ],
   "source": [
    "word_index = 0\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized')\n",
    "\n",
    "average_loss = 0\n",
    "num_steps = 10001\n",
    "\n",
    "\n",
    "for step in range(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch()\n",
    "    length = len(batch_inputs)\n",
    "    batch_inputs = np.array(batch_inputs)\n",
    "    batch_labels = np.array(batch_labels).reshape(length, 1)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "    _, loss_val = session.run([optimizer, nce_loss], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "        if step > 0:\n",
    "            average_loss /= 2000\n",
    "        # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "        print('Average loss at step ', step, ': ', average_loss)\n",
    "        average_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what an embedding looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_embeddings.shape =  (1000, 128)\n",
      "The first embedding vector is: \n",
      " [-0.06040489 -0.01330441 -0.09700806  0.09462865 -0.06287403  0.06696502\n",
      " -0.17774592 -0.0952784   0.12120966 -0.01762032  0.02647315 -0.08618247\n",
      "  0.16042912 -0.01304143 -0.12926026  0.07320725 -0.05332165  0.00240781\n",
      "  0.08730064  0.00759852 -0.03314163  0.0516535  -0.07823957  0.1113538\n",
      " -0.05894373  0.10860934  0.01588152  0.00113327  0.14620855 -0.17204115\n",
      " -0.05744638 -0.12161048 -0.03422917 -0.06331154  0.03345115 -0.00453212\n",
      " -0.00281025  0.07150039  0.0225572  -0.14017226 -0.01739253 -0.0360419\n",
      "  0.01315403  0.07402837 -0.06505622 -0.0322975   0.06845841 -0.03555855\n",
      " -0.10300234  0.01720936 -0.05083563 -0.07368746 -0.00757827 -0.00188826\n",
      " -0.15294185  0.03227917  0.15326868 -0.05712933 -0.26599848 -0.01704332\n",
      " -0.09562656 -0.05190606 -0.04794063  0.03382377  0.04194435  0.03574634\n",
      "  0.10957918 -0.03770536  0.03981107 -0.00529713  0.07970909  0.01328546\n",
      "  0.0946495   0.26669094 -0.0146098   0.00049715  0.02834046 -0.07508161\n",
      " -0.05904287  0.06665807 -0.1566963  -0.19649091  0.06735088  0.11043112\n",
      " -0.0048719   0.00974804  0.03240656  0.07711142 -0.13082755  0.01153916\n",
      "  0.04880192 -0.02027142  0.08838271  0.06448606  0.01367952 -0.03890909\n",
      "  0.01177586 -0.05366237 -0.12541521  0.13211116  0.04819062  0.09600776\n",
      "  0.11539903  0.04923315 -0.09568021  0.02945515  0.04348962  0.04577434\n",
      " -0.02105238 -0.14643234 -0.05643147 -0.04239038 -0.14784819  0.11097027\n",
      " -0.05933415  0.02210142 -0.06953997  0.00929612  0.10205241  0.20797458\n",
      " -0.11817178  0.02273543 -0.11592478 -0.06774038 -0.10758276  0.13138384\n",
      "  0.15169939  0.090183  ]\n"
     ]
    }
   ],
   "source": [
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "\n",
    "final_embeddings = normalized_embeddings.eval()\n",
    "print(\"final_embeddings.shape = \", final_embeddings.shape)\n",
    "print(\"The first embedding vector is: \\n\", final_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 65872), ('and', 36616), ('was', 33155), ('of', 25590), ('to', 22676), ('a', 18994), ('with', 16072), ('in', 14735), ('is', 10592), ('patient', 9822), ('she', 7581), ('for', 7548), ('were', 7177), ('no', 6987), ('he', 6477), ('on', 6339), ('this', 6211), ('at', 5818), ('then', 5775), ('right', 4904), ('as', 4881), ('that', 4720), ('left', 4716), ('has', 4713), ('her', 4495), ('or', 4138), ('history', 3950), ('there', 3716), ('had', 3559), ('procedure', 3549), ('be', 3530), ('his', 3438), ('placed', 3287), ('an', 3038), ('not', 3016), ('from', 2863), ('normal', 2753), ('well', 2719), ('pain', 2531), ('i', 2478), ('we', 2470), ('it', 2383), ('which', 2378), ('are', 2372), ('by', 2119), ('have', 2080), ('after', 2067), ('any', 2024), ('also', 1945), ('will', 1939), ('using', 1901), ('time', 1847), ('s', 1830), ('noted', 1814), ('into', 1798), ('mg', 1781), ('anesthesia', 1772), ('been', 1730), ('but', 1728), ('blood', 1719), ('incision', 1678), ('removed', 1673), ('performed', 1653), ('all', 1650), ('used', 1634), ('skin', 1613), ('room', 1613), ('without', 1453), ('diagnosis', 1441), ('some', 1419), ('through', 1374), ('artery', 1354), ('taken', 1343), ('anterior', 1279), ('general', 1245), ('position', 1236), ('over', 1175), ('back', 1167), ('up', 1163), ('fashion', 1149), ('area', 1135), ('past', 1116), ('who', 1100), ('made', 1095), ('closed', 1094), ('disease', 1058), ('out', 1054), ('two', 1047), ('postoperative', 1043), ('chest', 1034), ('lower', 1032), ('did', 1028), ('examination', 1026), ('down', 1025), ('vicryl', 1022), ('pressure', 1001), ('upper', 999), ('good', 995), ('operating', 990), ('preoperative', 977), ('does', 975), ('one', 971), ('x', 971), ('medical', 956), ('obtained', 954), ('lateral', 940), ('present', 935), ('suture', 935), ('approximately', 931), ('surgery', 924), ('condition', 921), ('given', 916), ('catheter', 911), ('sterile', 906), ('tissue', 904), ('both', 902), ('posterior', 900), ('significant', 898), ('medications', 871), ('prepped', 863), ('cm', 861), ('complications', 859), ('stable', 853), ('about', 850), ('care', 837), ('draped', 827), ('denies', 815), ('wound', 813), ('under', 812), ('physical', 807), ('done', 796), ('evidence', 796), ('heart', 793), ('very', 787), ('side', 782), ('loss', 779), ('neck', 774), ('small', 771), ('family', 766), ('clear', 764), ('none', 759), ('once', 751), ('coronary', 751), ('distal', 749), ('needle', 743), ('bilateral', 737), ('other', 735), ('identified', 727), ('next', 726), ('brought', 725), ('day', 723), ('negative', 723), ('symptoms', 716), ('seen', 715), ('rate', 709), ('intact', 707), ('head', 705), ('would', 702), ('prior', 697), ('tolerated', 696), ('exam', 694), ('mm', 691), ('surgical', 689), ('dissection', 686), ('further', 684), ('showed', 679), ('bleeding', 675), ('bladder', 675), ('abdomen', 674), ('hospital', 673), ('found', 672), ('female', 670), ('signs', 665), ('level', 664), ('than', 660), ('findings', 660), ('daily', 659), ('bilaterally', 658), ('these', 656), ('within', 656), ('status', 653), ('per', 647), ('discharge', 641), ('proximal', 638), ('chronic', 636), ('discussed', 635), ('if', 633), ('again', 633), ('applied', 633), ('acute', 632), ('abdominal', 631), ('plan', 630), ('evaluation', 627), ('more', 623), ('point', 622), ('tube', 622), ('bowel', 621), ('mild', 619), ('above', 619), ('medial', 617)]\n"
     ]
    }
   ],
   "source": [
    "print(words_counts.most_common(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[10, 14, 807, 853]\n",
      "['she', 'he', 'guide', 'release']\n"
     ]
    }
   ],
   "source": [
    "test_word = 'patient'\n",
    "\n",
    "print(dictionary[test_word])\n",
    "\n",
    "test_word_index = dictionary[test_word]\n",
    "test_word_embedding = final_embeddings[test_word_index:test_word_index+1, : ]\n",
    "\n",
    "#print(\"The embedding vector of test_word is: \\n\", test_word_embedding)\n",
    "\n",
    "top_k = 4\n",
    "test_word_top_k_similar = list((-test_word_embedding @ final_embeddings.T).argsort()[:, 1:top_k+1].flat)\n",
    "print( test_word_top_k_similar )\n",
    "print([reverse_dictionary[x] for x in test_word_top_k_similar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "5_word2vec.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
